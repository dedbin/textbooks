{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4hcfbzWLRGpt",
        "H70YI31vQPHn",
        "7TkgVubCIRbk",
        "lVFLVbKhK-cu",
        "HLJP14A1LNSK",
        "DKblziFlLtov",
        "rOzMjhxtMB5D",
        "hJN-ahmSM7L1",
        "bXenmHnVNqV5",
        "SrziWNsNOleh",
        "41HrJZzAO04J",
        "o0K_88-5Pt-2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Предобработка"
      ],
      "metadata": {
        "id": "4hcfbzWLRGpt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Предобработка данных** - это важный этап подготовки данных для обучения нейронной сети, который позволяет увеличить точность модели. Для этого используются различные методы, которые помогают оптимизировать данные, удалять шумы и выбросы, а также преобразовывать данные в удобный для нейронной сети формат.\n",
        "\n",
        "Один из методов предобработки данных - это *нормализация*, которая позволяет привести значения данных к одному диапазону, что упрощает обучение нейронной сети. Кроме того, можно использовать методы *фильтрации*, например, удаление выбросов или шумов.\n",
        "\n",
        "Для улучшения качества данных можно использовать методы *аугментации* данных, которые позволяют увеличить разнообразие тренировочных данных. Это могут быть такие методы, как изменение яркости, контраста, повороты изображения, изменение размеров и т.д.\n",
        "\n",
        "Важным этапом предобработки данных является также *выбор и подготовка признаков*, которые будут использоваться для обучения нейронной сети. Для этого можно использовать методы извлечения признаков или преобразования данных в удобный формат.\n",
        "\n",
        "В целом, предобработка данных - это важный этап подготовки данных для обучения нейронной сети, который позволяет увеличить точность модели и улучшить качество результатов. Для этого используются различные методы, в зависимости от типа данных и цели обучения нейронной сети."
      ],
      "metadata": {
        "id": "aqJZ5UMxH4ro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Библиотеки"
      ],
      "metadata": {
        "id": "H70YI31vQPHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. NumPy - это библиотека Python для работы с массивами и матрицами. Я использовал ее для работы с многомерными массивами данных и выполнения математических операций. Например, в одном из моих ответов я использовал функцию numpy.array() для преобразования списка в массив.\n",
        "\n",
        "2. Pandas - это библиотека Python для работы с данными в формате таблицы (DataFrame). Я использовал ее для чтения и записи данных в формате CSV, а также для выполнения операций над столбцами таблицы. Например, в одном из моих ответов я использовал функцию pandas.read_csv() для чтения данных из CSV-файла, а затем использовал операцию df['column_name'].mean() для вычисления среднего значения столбца.\n",
        "\n",
        "\n",
        "4. Scikit-learn - это библиотека Python для машинного обучения. Я использовал ее для создания моделей машинного обучения, оценки их качества и подготовки данных для обучения. Например, в одном из моих ответов я использовал функцию sklearn.model_selection.train_test_split() для разбиения данных на обучающую и тестовую выборки.\n",
        "\n",
        "5. Keras - это библиотека Python для создания и обучения нейронных сетей. Я использовал ее для создания моделей нейронных сетей и обучения их на данных. Например, в одном из моих ответов я использовал функцию keras.Sequential() для создания последовательной модели нейронной сети, а затем использовал функции model.fit() и model.predict() для обучения модели и получения предсказаний.\n",
        "\n",
        "6. NLTK - это библиотека Python для обработки естественного языка. Я использовал ее для выполнения операций над текстовыми данными, такими как токенизация, лемматизация и стемминг. Например, в одном из моих ответов я использовал функцию nltk.tokenize.word_tokenize() для токенизации текста.\n",
        "\n",
        "7. TensorFlow - это библиотека Python для создания и обучения нейронных сетей. Я использовал ее для создания и обучения нейронных сетей, а также для выполнения операций с тензорами. Например, в одном из моих ответов я использовал функции tensorflow.keras.layers.Dense() и tensorflow.keras.layers.Dropout() для создания слоев нейронной сети.\n",
        "\n",
        "8. PyTorch - это библиотека Python для создания и обучения нейронных сетей. Я использовал ее для создания моделей нейронных сетей и обучения их на данных. Например, в одном из моих ответов я использовал функцию torch.nn.Sequential() для создания последовательной модели нейронной сети, а затем использовал функции model.train() и model.eval() для обучения модели и получения предсказаний.\n"
      ],
      "metadata": {
        "id": "T27S3tK9QTqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cозданние dataframe\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7TkgVubCIRbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# создание таблицы данных\n",
        "data = {'name': ['Alice', 'Bob', 'Charlie', 'Dave'],\n",
        "        'age': [25, 30, 35, 40],\n",
        "        'salary': [50000, 70000, 90000, 110000],\n",
        "        'gender': ['female','male','female','male'],\n",
        "        }\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# сохранение таблицы в файл\n",
        "df.to_csv('data.csv', index=False)"
      ],
      "metadata": {
        "id": "busdx0MvIOje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Чтение данных"
      ],
      "metadata": {
        "id": "lVFLVbKhK-cu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('data.csv')\n",
        "print(data.head(6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS1hyv9ULGDx",
        "outputId": "9e28b241-2f16-42da-abb7-2c7ca9da435f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      name  age  salary  gender\n",
            "0    Alice   25   50000  female\n",
            "1      Bob   30   70000    male\n",
            "2  Charlie   35   90000  female\n",
            "3     Dave   40  110000    male\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В данном примере мы используем библиотеку Pandas для чтения данных из файла CSV. Наш датасет состоит из четырех признаков: возраст, пол, рост и вес. Для обработки данных мы можем применить несколько методов."
      ],
      "metadata": {
        "id": "XfOF0VEJLKTd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Нормализация данных"
      ],
      "metadata": {
        "id": "HLJP14A1LNSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "data['age_normalized'] = scaler.fit_transform(data[['age']])\n",
        "data['salary_normalized'] = scaler.fit_transform(data[['salary']])\n",
        "\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VIvd_-BLP5e",
        "outputId": "a059366c-d04c-4c42-b18f-629697cfa182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      name  age  salary  gender  age_normalized  salary_normalized\n",
            "0    Alice   25   50000  female        0.000000           0.000000\n",
            "1      Bob   30   70000    male        0.333333           0.333333\n",
            "2  Charlie   35   90000  female        0.666667           0.666667\n",
            "3     Dave   40  110000    male        1.000000           1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы использовали метод MinMaxScaler из библиотеки scikit-learn, чтобы привести значения признака \"рост\" к диапазону от 0 до 1."
      ],
      "metadata": {
        "id": "maGC4YB4Lskm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Удаление выбросов"
      ],
      "metadata": {
        "id": "DKblziFlLtov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def remove_outliers(data, feature):\n",
        "    Q1 = np.percentile(data[feature], 25)\n",
        "    Q3 = np.percentile(data[feature], 75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    data = data[(data[feature] > lower_bound) & (data[feature] < upper_bound)]\n",
        "    return data\n",
        "\n",
        "data = remove_outliers(data, 'salary')\n",
        "\n",
        "print(data.head(6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osDRwDAtLv6W",
        "outputId": "56cb666f-dae1-4c83-f0ee-0631dfc27940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      name  age  salary  gender  age_normalized  salary_normalized\n",
            "0    Alice   25   50000  female        0.000000           0.000000\n",
            "1      Bob   30   70000    male        0.333333           0.333333\n",
            "2  Charlie   35   90000  female        0.666667           0.666667\n",
            "3     Dave   40  110000    male        1.000000           1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В функции remove_outliers мы использовали квартили и межквартильный размах, чтобы определить границы выбросов. Затем мы удалили строки, содержащие выбросы, и вернули очищенный датасет."
      ],
      "metadata": {
        "id": "xYZN_8xTL_II"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Кодирование категориальных признаков"
      ],
      "metadata": {
        "id": "rOzMjhxtMB5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "gender_encoded = pd.DataFrame(encoder.fit_transform(data[['gender']]).toarray(), columns=['female', 'male'])\n",
        "data = pd.concat([data, gender_encoded], axis=1)\n",
        "\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZ-v70LWME0B",
        "outputId": "6810e96d-fddb-439f-cfd5-9c476d42e141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      name  age  salary  gender  female  male\n",
            "0    Alice   25   50000  female     1.0   0.0\n",
            "1      Bob   30   70000    male     0.0   1.0\n",
            "2  Charlie   35   90000  female     1.0   0.0\n",
            "3     Dave   40  110000    male     0.0   1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы использовали метод OneHotEncoder из библиотеки scikit-learn, чтобы закодировать категориальный признак \"пол\" в двоичный формат. Затем мы объединили закодированный признак с исходным датасетом с помощью метода concat.\n",
        "\n",
        "Это некоторые из методов предобработки данных, которые мы рассмотрели на примере Python. Они могут быть очень полезны при подготовке данных для обучения нейросетей и помогают улучшить качество модели."
      ],
      "metadata": {
        "id": "BvV9j19rMiK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Подготовка текста"
      ],
      "metadata": {
        "id": "8zRIsJFzMk5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Очистка текста от шума и нежелательных символов. Нежелательными символами могут быть знаки препинания, цифры, артикли и другие слова, которые не несут смысловой нагрузки.\n",
        "\n",
        "2. Токенизация текста - разделение текста на отдельные слова или фразы (токены).\n",
        "\n",
        "3. Построение словаря - создание списка всех уникальных слов, которые встречаются в тексте.\n",
        "\n",
        "4. Преобразование текста в числовую форму - каждому слову из словаря присваивается уникальный числовой код.\n",
        "\n",
        "5. Создание последовательностей - текст разбивается на последовательности фиксированной длины.\n",
        "\n",
        "6. Подготовка обучающей и тестовой выборок.\n",
        "\n",
        "Рассмотрим каждый шаг подробнее."
      ],
      "metadata": {
        "id": "uH_spBuIMvh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Очистка текста"
      ],
      "metadata": {
        "id": "hJN-ahmSM7L1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перед токенизацией необходимо очистить текст от шума и нежелательных символов. Для этого можно использовать регулярные выражения или библиотеку nltk."
      ],
      "metadata": {
        "id": "Djk-N-etM9SF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "#--------#\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from pymystem3 import Mystem\n",
        "from string import punctuation\n",
        "\n",
        "#Create lemmatizer and stopwords list\n",
        "mystem = Mystem()\n",
        "russian_stopwords = stopwords.words(\"russian\")\n",
        "\n",
        "#Preprocess function\n",
        "def preprocess_text(text):\n",
        "    tokens = mystem.lemmatize(text.lower())\n",
        "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
        "              and token != \" \" \\\n",
        "              and token.strip() not in punctuation]\n",
        "\n",
        "    text = \" \".join(tokens)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U11D9wSxM-ww",
        "outputId": "452b9790-e20a-4cb1-e2ae-ec903481fec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Токенизация"
      ],
      "metadata": {
        "id": "bXenmHnVNqV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенизация - разделение текста на отдельные слова или фразы (токены). Для этого можно использовать библиотеку nltk."
      ],
      "metadata": {
        "id": "jtlw45NsNuJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ier2ljRjOZNj",
        "outputId": "f324a1a4-f506-425d-a8e0-3d2dd5153372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"This is an example sentence.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME5IhEraOZqY",
        "outputId": "b813c2ac-73d0-4c03-ca37-fe8f6d3d7eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'an', 'example', 'sentence', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Преобразование текста в числовую форму"
      ],
      "metadata": {
        "id": "SrziWNsNOleh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Каждому слову из словаря присваивается уникальный числовой код. Это делается с помощью токенизации и присвоения каждому слову своего индекса в словаре."
      ],
      "metadata": {
        "id": "Sy8WCiPZOoVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(tokens)\n",
        "\n",
        "# преобразование текста в последовательность чисел\n",
        "sequences = tokenizer.texts_to_sequences(tokens)\n",
        "\n",
        "# словарь, соответствие слов и их индексов\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNr4hkb9Obqs",
        "outputId": "29fd4190-19c0-4438-a353-0c46ac5a8eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5 unique tokens.\n",
            "[[1], [2], [3], [4], [5], []]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Создание последовательностей"
      ],
      "metadata": {
        "id": "41HrJZzAO04J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Текст разбивается на последовательности фиксированной длины. Для этого можно использовать функцию pad_sequences из библиотеки keras."
      ],
      "metadata": {
        "id": "56oUGckHO3bL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import padsequences\n",
        "\n",
        "maxlen = 100  # максимальная длина последовательности\n",
        "\n",
        "# создание последовательностей\n",
        "data = padsequences(sequences, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "i8Yfgc-wO44b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Подготовка обучающей и тестовой выборок"
      ],
      "metadata": {
        "id": "o0K_88-5Pt-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тексты необходимо разделить на обучающую и тестовую выборки. Обучающая выборка используется для обучения нейросети, тестовая - для проверки качества модели."
      ],
      "metadata": {
        "id": "fn0eIqYGPxMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "labels = np.array(labels)\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inOUsvd8Py3X",
        "outputId": "35834546-9c90-45e2-97f5-efdfc384c201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ]
}